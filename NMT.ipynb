{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Neural Machine Translation**\n",
    "\n",
    "1. Neural Machine Translation (NMT) is the task of using artificial neural network models for translation from one language to the other.\n",
    "\n",
    "2. The NMT model generally consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation.\n",
    "\n",
    "3. This problem can be thought as a prediction problem, where given a sequence of words in source language as input, task is to predict the output sequence of words in target language.\n",
    "\n",
    "4. The dataset comes from http://www.manythings.org/anki/, where you may find tab delimited bilingual sentence pairs in different files based on the source and target language of your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import required libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Onc3iU6FJRb3"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IVn57R7pYNRz"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8HWG7qTyWHap"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM,Bidirectional\n",
    "from keras.layers import Dense,GRU,Input, Concatenate, Attention\n",
    "from keras.layers import Embedding,Add\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed,Input\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LXIqWgpzgTzc"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step-1: Download and clean the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "k08kz-cqj9Hf"
   },
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    def __init__(self, file_path, split_per,dataset_size):\n",
    "        self.file_path = file_path\n",
    "        self.split_per = split_per\n",
    "        self.dataset_size=dataset_size\n",
    "        \n",
    "    def load_raw_data(self):\n",
    "        with open(self.file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        lines = text.splitlines()\n",
    "        self.raw_context = [line.split('\\t')[0] for line in lines]\n",
    "        self.raw_target = [line.split('\\t')[1] for line in lines]\n",
    "        return self.raw_target, self.raw_context\n",
    "\n",
    "    def clean(self, t):\n",
    "        s = set(string.punctuation)\n",
    "        cleaned = []\n",
    "        x = \"startseq\"\n",
    "        for word in word_tokenize(t):\n",
    "            if (word.lower() not in s):\n",
    "                cleaned.append(word.lower())\n",
    "        for i in cleaned:\n",
    "            if i == cleaned[len(cleaned) - 1]:\n",
    "                x = x + \" \" + i + \" endseq\"\n",
    "            else:\n",
    "                x = x + \" \" + i\n",
    "        return x\n",
    "\n",
    "    def cleaned_data(self):\n",
    "        cleaned_target = [self.clean(i) for i in self.raw_target]\n",
    "        cleaned_context = [self.clean(i) for i in self.raw_context]\n",
    "        self.target=np.array(cleaned_target[:self.dataset_size])\n",
    "        self.context=np.array(cleaned_context[:self.dataset_size])\n",
    "\n",
    "    def train_test_split(self):\n",
    "        self.vocab()\n",
    "        index=int(self.split_per*len(self.sequence_target))\n",
    "        indices= np.arange(len(self.sequence_target))\n",
    "        np.random.shuffle(indices)\n",
    "        training_indices=indices[:index]\n",
    "        testing_indices=indices[index:]\n",
    "        self.train_target=self.sequence_target[training_indices]\n",
    "        self.test_target=self.sequence_target[testing_indices]\n",
    "        self.test_raw_test=self.target[testing_indices]\n",
    "        self.train_context=self.sequence_context[training_indices]\n",
    "        self.testing_context=self.sequence_context[testing_indices]\n",
    "        return  self.train_target, self.test_target,self.train_context,self.testing_context\n",
    "\n",
    "\n",
    "    def tokenize(self):\n",
    "        tokenizer = Tokenizer(oov_token='<UNK>')\n",
    "        return tokenizer\n",
    "\n",
    "    def vocab(self):\n",
    "        self.cleaned_data()\n",
    "        self.token_target = self.tokenize()\n",
    "        self.token_context = self.tokenize()\n",
    "        self.token_target.fit_on_texts(self.target)\n",
    "        self.token_context.fit_on_texts(self.context)\n",
    "        self.vocab_target = self.token_target.word_index.keys()\n",
    "        self.vocab_context = self.token_context.word_index.keys()\n",
    "        self.sequence_target = np.array(self.token_target.texts_to_sequences(self.target),dtype=object)\n",
    "        self.sequence_context = np.array(self.token_context.texts_to_sequences(self.context),dtype=object)\n",
    "\n",
    "    def max_seq_len(self, input_sequences):\n",
    "        max_seq_len = max([len(seq) for seq in input_sequences])\n",
    "        return max_seq_len\n",
    "\n",
    "    def pad_sequence(self,x,lang):\n",
    "      if lang=='target':\n",
    "        max_seq_len = self.max_seq_len(self.sequence_target)\n",
    "      elif lang=='context':\n",
    "        max_seq_len = self.max_seq_len(self.sequence_context)\n",
    "      padded_sequences = np.array(pad_sequences(x, maxlen=max_seq_len))\n",
    "      return padded_sequences\n",
    "\n",
    "    def flatten_sequence(self, sequences):\n",
    "        return [item for sublist in sequences for item in sublist]\n",
    "\n",
    "    def one_hot_encode(self, targets, vocab_size):\n",
    "        # flattened_targets = self.flatten_sequence(targets)\n",
    "        one_hot_targets = to_categorical(targets, num_classes=vocab_size)\n",
    "        return one_hot_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "X7xF1zL7Rigb"
   },
   "outputs": [],
   "source": [
    "file_path=\"fra-eng/fra.txt\"\n",
    "preprocessor=Preprocess(file_path,0.7,20000)\n",
    "raw_target, raw_context=preprocessor.load_raw_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step-2: Split and Prepare the Data for Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "n3RmR9Huichg"
   },
   "outputs": [],
   "source": [
    "train_target,test_target,train_context,test_context = preprocessor.train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3zyTaQ-uN0_U",
    "outputId": "2b31e322-d30a-412e-941d-f709d161d228"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "HQn6KgiWdUFJ"
   },
   "outputs": [],
   "source": [
    "padded_train_target=preprocessor.pad_sequence(train_target,'target')\n",
    "padded_test_target=preprocessor.pad_sequence(test_target,'target')\n",
    "padded_train_context=preprocessor.pad_sequence(train_context,'context')\n",
    "padded_test_context=preprocessor.pad_sequence(test_context,'context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aiA37OHoXkTI",
    "outputId": "47a5f238-b3a9-4652-e7de-698f50869772"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    3,\n",
       "         14, 1262,    2])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_train_target[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "2oyQRLzmmgzM"
   },
   "outputs": [],
   "source": [
    "target_vocab_size=len(preprocessor.vocab_target)+1\n",
    "context_vocab_size=len(preprocessor.vocab_context)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3280"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "FMvdM0lYnAaN"
   },
   "outputs": [],
   "source": [
    "trainY = preprocessor.one_hot_encode(padded_train_target, target_vocab_size)\n",
    "testY=preprocessor.one_hot_encode(padded_test_target, target_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 3: Define and Train the RNN-based Encoder-Decoder Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "uyh5KP8MJ826"
   },
   "outputs": [],
   "source": [
    "def define_model(context_vocab, target_vocab, context_timesteps, target_timesteps, n_units):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(context_vocab, n_units, input_length=context_timesteps, mask_zero=True))\n",
    "    model.add(Bidirectional(GRU(n_units)))\n",
    "    model.add(RepeatVector(target_timesteps))\n",
    "    model.add(LSTM(n_units, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(target_vocab, activation='softmax')))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "mch4_LloUxWG"
   },
   "outputs": [],
   "source": [
    "max_seq_len_context=preprocessor.max_seq_len(preprocessor.sequence_context)\n",
    "max_seq_len_target=preprocessor.max_seq_len(preprocessor.sequence_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "kKV4MUxvoNVs"
   },
   "outputs": [],
   "source": [
    "\n",
    "model = define_model(context_vocab_size, target_vocab_size, max_seq_len_context, max_seq_len_target, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iTA0Qw3OegAY",
    "outputId": "9e0fba61-450e-42d9-f45c-fe4b3c8e81ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([3, 17, 8, 910, 2]), list([3, 5, 11, 88, 299, 2]),\n",
       "       list([3, 40, 8, 17, 2]), ..., list([3, 4, 12, 88, 319, 2]),\n",
       "       list([3, 57, 58, 61, 2]), list([3, 4, 64, 22, 412, 2])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kqBIa994eilG",
    "outputId": "78437a59-0553-46cf-8c41-e48eaed24fa4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14000, 14, 6543)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "INEAQpBtaVc6",
    "outputId": "5c98357e-8039-430c-a683-8585df61d089"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 8, 256)            839680    \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirecti  (None, 512)               789504    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " repeat_vector_2 (RepeatVec  (None, 14, 512)           0         \n",
      " tor)                                                            \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 14, 256)           787456    \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDi  (None, 14, 6543)          1681551   \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4098191 (15.63 MB)\n",
      "Trainable params: 4098191 (15.63 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "219/219 [==============================] - ETA: 0s - loss: 2.1971\n",
      "Epoch 1: val_loss improved from inf to 2.17122, saving model to model.tf\n",
      "INFO:tensorflow:Assets written to: model.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 60s 238ms/step - loss: 2.1971 - val_loss: 2.1712\n",
      "Epoch 2/30\n",
      "219/219 [==============================] - ETA: 0s - loss: 2.1138\n",
      "Epoch 2: val_loss improved from 2.17122 to 2.13954, saving model to model.tf\n",
      "INFO:tensorflow:Assets written to: model.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 51s 234ms/step - loss: 2.1138 - val_loss: 2.1395\n",
      "Epoch 3/30\n",
      "219/219 [==============================] - ETA: 0s - loss: 2.0732\n",
      "Epoch 3: val_loss improved from 2.13954 to 2.09631, saving model to model.tf\n",
      "INFO:tensorflow:Assets written to: model.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 56s 256ms/step - loss: 2.0732 - val_loss: 2.0963\n",
      "Epoch 4/30\n",
      "219/219 [==============================] - ETA: 0s - loss: 2.0425\n",
      "Epoch 4: val_loss improved from 2.09631 to 2.06754, saving model to model.tf\n",
      "INFO:tensorflow:Assets written to: model.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 58s 266ms/step - loss: 2.0425 - val_loss: 2.0675\n",
      "Epoch 5/30\n",
      "219/219 [==============================] - ETA: 0s - loss: 2.0133\n",
      "Epoch 5: val_loss improved from 2.06754 to 2.06365, saving model to model.tf\n",
      "INFO:tensorflow:Assets written to: model.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 66s 301ms/step - loss: 2.0133 - val_loss: 2.0637\n",
      "Epoch 6/30\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.9930\n",
      "Epoch 6: val_loss did not improve from 2.06365\n",
      "219/219 [==============================] - 60s 272ms/step - loss: 1.9930 - val_loss: 2.0884\n",
      "Epoch 7/30\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.9722\n",
      "Epoch 7: val_loss improved from 2.06365 to 2.00949, saving model to model.tf\n",
      "INFO:tensorflow:Assets written to: model.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 75s 343ms/step - loss: 1.9722 - val_loss: 2.0095\n",
      "Epoch 8/30\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.9580\n",
      "Epoch 8: val_loss improved from 2.00949 to 2.00280, saving model to model.tf\n",
      "INFO:tensorflow:Assets written to: model.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 51s 232ms/step - loss: 1.9580 - val_loss: 2.0028\n",
      "Epoch 9/30\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.9381\n",
      "Epoch 9: val_loss did not improve from 2.00280\n",
      "219/219 [==============================] - 41s 190ms/step - loss: 1.9381 - val_loss: 2.0180\n",
      "Epoch 10/30\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.9255\n",
      "Epoch 10: val_loss improved from 2.00280 to 1.97575, saving model to model.tf\n",
      "INFO:tensorflow:Assets written to: model.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 54s 249ms/step - loss: 1.9255 - val_loss: 1.9757\n",
      "Epoch 11/30\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.9109\n",
      "Epoch 11: val_loss did not improve from 1.97575\n",
      "219/219 [==============================] - 57s 260ms/step - loss: 1.9109 - val_loss: 1.9763\n",
      "Epoch 12/30\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.8988\n",
      "Epoch 12: val_loss did not improve from 1.97575\n",
      "219/219 [==============================] - 58s 265ms/step - loss: 1.8988 - val_loss: 1.9807\n",
      "Epoch 13/30\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.8829\n",
      "Epoch 13: val_loss improved from 1.97575 to 1.95270, saving model to model.tf\n",
      "INFO:tensorflow:Assets written to: model.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 82s 375ms/step - loss: 1.8829 - val_loss: 1.9527\n",
      "Epoch 14/30\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.8701\n",
      "Epoch 14: val_loss improved from 1.95270 to 1.94319, saving model to model.tf\n",
      "INFO:tensorflow:Assets written to: model.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 88s 403ms/step - loss: 1.8701 - val_loss: 1.9432\n",
      "Epoch 15/30\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.8633\n",
      "Epoch 15: val_loss improved from 1.94319 to 1.92921, saving model to model.tf\n",
      "INFO:tensorflow:Assets written to: model.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 72s 329ms/step - loss: 1.8633 - val_loss: 1.9292\n",
      "Epoch 16/30\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.8439\n",
      "Epoch 16: val_loss improved from 1.92921 to 1.92050, saving model to model.tf\n",
      "INFO:tensorflow:Assets written to: model.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 72s 328ms/step - loss: 1.8439 - val_loss: 1.9205\n",
      "Epoch 17/30\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.8312\n",
      "Epoch 17: val_loss did not improve from 1.92050\n",
      "219/219 [==============================] - 83s 378ms/step - loss: 1.8312 - val_loss: 1.9374\n",
      "Epoch 18/30\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.8203\n",
      "Epoch 18: val_loss did not improve from 1.92050\n",
      "219/219 [==============================] - 83s 381ms/step - loss: 1.8203 - val_loss: 1.9277\n",
      "Epoch 19/30\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.8126\n",
      "Epoch 19: val_loss improved from 1.92050 to 1.90775, saving model to model.tf\n",
      "INFO:tensorflow:Assets written to: model.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 93s 426ms/step - loss: 1.8126 - val_loss: 1.9077\n",
      "Epoch 20/30\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.7903\n",
      "Epoch 20: val_loss did not improve from 1.90775\n",
      "219/219 [==============================] - 86s 392ms/step - loss: 1.7903 - val_loss: 1.9144\n",
      "Epoch 21/30\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.7789\n",
      "Epoch 21: val_loss improved from 1.90775 to 1.88484, saving model to model.tf\n",
      "INFO:tensorflow:Assets written to: model.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 100s 456ms/step - loss: 1.7789 - val_loss: 1.8848\n",
      "Epoch 22/30\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.7723\n",
      "Epoch 22: val_loss improved from 1.88484 to 1.86487, saving model to model.tf\n",
      "INFO:tensorflow:Assets written to: model.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 83s 378ms/step - loss: 1.7723 - val_loss: 1.8649\n",
      "Epoch 23/30\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.7527\n",
      "Epoch 23: val_loss did not improve from 1.86487\n",
      "219/219 [==============================] - 78s 358ms/step - loss: 1.7527 - val_loss: 1.8852\n",
      "Epoch 24/30\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.7430\n",
      "Epoch 24: val_loss improved from 1.86487 to 1.84941, saving model to model.tf\n",
      "INFO:tensorflow:Assets written to: model.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 91s 414ms/step - loss: 1.7430 - val_loss: 1.8494\n",
      "Epoch 25/30\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.7266\n",
      "Epoch 25: val_loss did not improve from 1.84941\n",
      "219/219 [==============================] - 128s 587ms/step - loss: 1.7266 - val_loss: 1.8708\n",
      "Epoch 26/30\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.7297\n",
      "Epoch 26: val_loss improved from 1.84941 to 1.83690, saving model to model.tf\n",
      "INFO:tensorflow:Assets written to: model.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 142s 649ms/step - loss: 1.7297 - val_loss: 1.8369\n",
      "Epoch 27/30\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.7099\n",
      "Epoch 27: val_loss did not improve from 1.83690\n",
      "219/219 [==============================] - 76s 346ms/step - loss: 1.7099 - val_loss: 1.8437\n",
      "Epoch 28/30\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.6991\n",
      "Epoch 28: val_loss improved from 1.83690 to 1.81650, saving model to model.tf\n",
      "INFO:tensorflow:Assets written to: model.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 87s 396ms/step - loss: 1.6991 - val_loss: 1.8165\n",
      "Epoch 29/30\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.6792\n",
      "Epoch 29: val_loss did not improve from 1.81650\n",
      "219/219 [==============================] - 74s 339ms/step - loss: 1.6792 - val_loss: 1.8319\n",
      "Epoch 30/30\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.6709\n",
      "Epoch 30: val_loss did not improve from 1.81650\n",
      "219/219 [==============================] - 75s 341ms/step - loss: 1.6709 - val_loss: 1.8285\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x26aad3feec0>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "# summarize defined model\n",
    "print(model.summary())\n",
    "plot_model(model, to_file='model.png', show_shapes=True)\n",
    "# fit model\n",
    "filename = 'model.tf'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "\n",
    "\n",
    "model.fit(padded_train_context, trainY, epochs=30, batch_size=64, validation_data=(padded_test_context, testY), callbacks=[checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Bihw7XKk7zw",
    "outputId": "a5070de3-3b94-40b2-a809-3a5f22a2dbba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step-4 : Evaluate The Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UL3OuOnoRYhC",
    "outputId": "bc2aae1d-3579-43f5-9073-0a494b4808b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.617742\n",
      "BLEU-2: 0.407798\n",
      "BLEU-3: 0.315834\n",
      "BLEU-4: 0.188856\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "    prediction = model.predict(source, verbose=0)[0]\n",
    "    integers = [np.argmax(vector) for vector in prediction]\n",
    "    target = []\n",
    "    for i in integers:\n",
    "        word = word_for_id(i, tokenizer)\n",
    "        if word is None:\n",
    "            continue  # Skip None values\n",
    "        target.append(word)\n",
    "    return ' '.join(target)  # Return None for empty sequences\n",
    "\n",
    "\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "def evaluate_model(model, tokenizer, sources, raw_targets):\n",
    "    actual, predicted = list(), list()\n",
    "    for i, source in enumerate(sources):\n",
    "        # Translate encoded source text\n",
    "        source = source.reshape((1, source.shape[0]))\n",
    "        translation = predict_sequence(model, tokenizer, source)\n",
    "        if translation is not None:\n",
    "            raw_target = raw_targets[i]\n",
    "            actual.append([raw_target.split()])  # Split the reference sentence into words\n",
    "            predicted.append(translation.split())  # Split the predicted sentence into words\n",
    "    if not actual or not predicted:\n",
    "        print(\"No valid translations found.\")\n",
    "        return None, None\n",
    "    # Calculate BLEU score\n",
    "    print('BLEU-1:' , corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2:' , corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3:' , corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4:' , corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "    return actual, predicted\n",
    "\n",
    "\n",
    "model=load_model('model.tf')\n",
    "\n",
    "actual, predicted = evaluate_model(model, preprocessor.token_target, padded_test_context, preprocessor.test_raw_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l-xWQBB5jsiV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
